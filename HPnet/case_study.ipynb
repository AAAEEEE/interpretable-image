{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import argparse\n",
    "from helpers import makedir, adjust_learning_rate\n",
    "import model\n",
    "import push\n",
    "import train_and_test as tnt\n",
    "import save\n",
    "from log import create_logger\n",
    "from preprocess import mean, std, preprocess_input_function, img_size\n",
    "from node import Node\n",
    "import time\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "data_path = \"../datasets/imagenet/\"\n",
    "model_path = \"saved_models_8protos1/\"\n",
    "resume_path =  model_path + \"best_model_last_opt.pth\"\n",
    "\n",
    "batch_size = 50\n",
    "n_protos_per_class = 8\n",
    "proto_dim = 32\n",
    "\n",
    "\n",
    "# load the data\n",
    "data_path = data_path\n",
    "train_dir = data_path + 'train/'\n",
    "valid_dir = data_path + 'valid/'\n",
    "test_dir = data_path + 'test/'\n",
    "OOD_dir = data_path + 'OODall/test'\n",
    "train_push_dir = train_dir\n",
    "train_batch_size = batch_size\n",
    "valid_batch_size = batch_size\n",
    "test_batch_size = batch_size\n",
    "train_push_batch_size = batch_size\n",
    "\n",
    "# dataset setup\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "\ttransforms.Resize(256),\n",
    "\ttransforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "])\n",
    "\n",
    "\n",
    "# train set\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transform_test)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=False)\n",
    "# valid set\n",
    "valid_dataset = datasets.ImageFolder(\n",
    "    valid_dir,\n",
    "    transform_test)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=valid_batch_size, shuffle=False)    \n",
    "# test set\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "# OOD set\n",
    "OOD_dataset = datasets.ImageFolder(\n",
    "    OOD_dir,\n",
    "    transform_test)\n",
    "OOD_loader = torch.utils.data.DataLoader(\n",
    "    OOD_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "print('training set size: {0}'.format(len(train_loader.dataset)))\n",
    "print('valid set size: {0}'.format(len(valid_loader.dataset)))\n",
    "print('test set size: {0}'.format(len(test_loader.dataset)))\t\t\n",
    "print('batch size: {0}'.format(train_batch_size))\n",
    "\n",
    "\n",
    "# construct the tree\n",
    "root = Node(\"root\")\n",
    "root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "root.add_children_to('animal',['non_primate','primate'])\n",
    "root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "root.assign_proto_dirs()\n",
    "\n",
    "\n",
    "OODroot = Node(\"root\")\n",
    "OODroot.add_children(['animal','vehicle','everyday_object','weapon',\"scuba_diver\"])\n",
    "OODroot.add_children_to('animal',['non_primate','primate'])\n",
    "OODroot.add_children_to('non_primate',['king_penguin','tree_frog','zebra'])\n",
    "OODroot.add_children_to('primate',['macaque','gorilla','chimpanzee'])\n",
    "OODroot.add_children_to('vehicle',['cab','forklift','tractor','mountain_bike'])\n",
    "OODroot.add_children_to('everyday_object',['golf_ball','wallet','table_lamp'])\n",
    "OODroot.add_children_to('weapon',['revolver','bow'])\n",
    "OODroot.assign_all_descendents()\n",
    "OODroot.assign_proto_dirs()\n",
    "\n",
    "\n",
    "IDcoarse_names = root.children_names()\n",
    "IDfine_names = os.listdir(train_dir)\n",
    "IDfine_names.sort()\n",
    "label2name = {i : name for (i,name) in enumerate(IDfine_names)}\n",
    "IDfineLabel2coarseLabel = {label : root.children_to_labels[root.closest_descendent_for(name).name] for label, name in enumerate(IDfine_names)}    \n",
    "\n",
    "OODcoarse_names = OODroot.children_names()\n",
    "OODfine_names = os.listdir(OOD_dir)\n",
    "OODfine_names.sort()\n",
    "OODfineLabel2coarseLabel = {label : OODroot.children_to_labels[OODroot.closest_descendent_for(name).name] for label, name in enumerate(OODfine_names)}\n",
    "\n",
    "num_fine = len(IDfine_names)\n",
    "num_coarse = len(root.children)\n",
    "\n",
    "\n",
    "vgg = model.vgg16_proto(root, pretrained=True, num_prototypes_per_class=n_protos_per_class,prototype_dimension=proto_dim, img_size=img_size, resume_path = resume_path)\n",
    "vgg = vgg.cuda()\n",
    "vgg_multi = torch.nn.DataParallel(vgg)\n",
    "class_specific=True\n",
    "\n",
    "\n",
    "save = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = tnt.test(model=vgg_multi, dataloader=test_loader, label2name=label2name, class_specific=class_specific, log=print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_with_children = [node.name for node in vgg.root.nodes_with_children()]\n",
    "for name in names_with_children:\n",
    "    print('\\n' + name)\n",
    "    layer = getattr(vgg,name+\"_layer\")\n",
    "    weights = [p.data for p in layer.parameters()][0]\n",
    "    weights = np.array([[np.round(weight.item(),2) for weight in beta] for beta in weights])\n",
    "    print(weights)\n",
    "    #print(np.linalg.norm(weights))\n",
    "    #print([[l > 0 for l in p.data] for p in layer.parameters()]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_vecs = vgg.root_prototype_vectors.detach().cpu().numpy()\n",
    "for i in range(30):\n",
    "    print(IDcoarse_names[i//6])\n",
    "    print([np.round(x,2) for x in root_vecs[i,:,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for node in root.nodes_with_children():\n",
    "    path = model_path + node.proto_dir\n",
    "    prototype_info = np.load(os.path.join(path, 'bb'+'.npy'))\n",
    "    #if node.name == \"root\": print(prototype_info)\n",
    "    setattr(node,\"proto_idx\",prototype_info[:,0])\n",
    "    \n",
    "def img_id_to_name(idx):\n",
    "    per_class = 1250\n",
    "    class_id = idx // per_class\n",
    "    return IDfine_names[class_id]\n",
    "\n",
    "node = root.get_node(\"root\")\n",
    "\n",
    "for i, proto_id in enumerate(node.proto_idx):\n",
    "    print(str(i) + \" \" + img_id_to_name(proto_id))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick an ID batch to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize \n",
    "from preprocess import undo_preprocess_input_function    \n",
    "\n",
    "def plot_preprocessed_img(preprocessed_imgs, index):\n",
    "    img_copy = copy.deepcopy(preprocessed_imgs[index:index+1])\n",
    "    undo_preprocessed_img = undo_preprocess_input_function(img_copy)\n",
    "    print('image index {0} in batch'.format(index))\n",
    "    undo_preprocessed_img = np.transpose(undo_preprocessed_img[0], [1,2,0])    \n",
    "    plt.imshow(undo_preprocessed_img.numpy())\n",
    "    #plt.imshow(undo_preprocessed_img)\n",
    "    plt.show()\n",
    "    #del img_copy\n",
    "    #del undo_preprocessed_img\n",
    "    return undo_preprocessed_img\n",
    "\n",
    "def show_prototype(img_dir,index,original=False):\n",
    "    p_img = plt.imread(os.path.join(img_dir, 'prototype-img'+str(index)+'.png'))\n",
    "    if original: p_img = plt.imread(os.path.join(img_dir, 'prototype-original-img'+str(index)+'.png'))\n",
    "    d1 = p_img.shape[0]\n",
    "    d2 = p_img.shape[1]\n",
    "    #p_img = resize(p_img,[d1*2,d2*2,3])\n",
    "    plt.imshow(p_img)\n",
    "    plt.show()\n",
    "    return p_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(x,y) for (x,y) in enumerate(IDfine_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2 # 50 images per class\n",
    "batch_dataset = torch.utils.data.Subset(test_dataset, range(i*test_batch_size,(i+1)*test_batch_size))\n",
    "sample_size = 10\n",
    "batch_loader = torch.utils.data.DataLoader(\n",
    "    batch_dataset, batch_size=sample_size, shuffle=False)\n",
    "\n",
    "iter_batch_loader = iter(batch_loader)\n",
    "\n",
    "quintile = 2\n",
    "\n",
    "for i in range(quintile):\n",
    "    images, labels = iter_batch_loader.next()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = plot_preprocessed_img(images, 0)\n",
    "\n",
    "for i in range(10):\n",
    "    _ = plot_preprocessed_img(images, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"weapon\"\n",
    "\n",
    "eps = vgg.epsilon\n",
    "def activations(min_distances,eps=eps):\n",
    "    return torch.log(1 + (1 / (min_distances + eps)))\n",
    "def activation_patterns(model, x, name, eps=eps):\n",
    "    conv_features = model.conv_features(x)\n",
    "    distances = model.prototype_distances(conv_features,name)\n",
    "    return torch.log(1 + (1 / (distances + eps)))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    images_cuda = images.cuda()\n",
    "    _ = vgg(images_cuda)\n",
    "                    \n",
    "root_y = torch.tensor([IDfineLabel2coarseLabel[y.item()] for y in labels]).cuda()\n",
    "\n",
    "preds_root, preds_joint = vgg.get_joint_distribution()\n",
    "\n",
    "coarse_preds = torch.argmax(preds_root, dim=1)\n",
    "fine_preds = torch.argmax(preds_joint, dim=1)\n",
    "\n",
    "root_activations = activations(vgg.root.min_distances)\n",
    "root_activation_patterns = activation_patterns(vgg, images_cuda, \"root\")\n",
    "case_activations = activations(vgg.root.get_node(case).min_distances)\n",
    "case_activation_patterns = activation_patterns(vgg, images_cuda, case)\n",
    "\n",
    "coarse_correct = (coarse_preds == root_y)\n",
    "fine_correct = (fine_preds == labels.cuda())\n",
    "            \n",
    "print(\"Batch coarse accuracy: %.2f\" % (coarse_correct.sum().item() / len(labels)))\n",
    "print(\"Indices of wrong coarse preds: \", np.where(coarse_correct == 0)[0])\n",
    "print(\"Batch fine accuracy: %.2f\" % (fine_correct.sum().item() / len(labels)))\n",
    "print(\"Indices of wrong fine preds: \", np.where(fine_correct == 0)[0])\n",
    "\n",
    "del images_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx_list = np.arange(10)\n",
    "idx_list = [1]\n",
    "import cv2\n",
    "\n",
    "case_node = vgg.root.get_node(case)\n",
    "\n",
    "for idx in idx_list:\n",
    "        \n",
    "    \n",
    "    fine_pred_name = IDfine_names[fine_preds[idx]]\n",
    "    fine_true_name = IDfine_names[labels[idx]]\n",
    "    coarse_pred_name = IDcoarse_names[coarse_preds[idx]]    \n",
    "    coarse_true_name = IDcoarse_names[root_y[idx]]\n",
    "    print(\"\\nlabel: \" + fine_true_name + \", \" + coarse_true_name,\"\\npredictions: \" + fine_pred_name + \", \" + coarse_pred_name)\n",
    "    original_img = plot_preprocessed_img(images, idx).numpy()\n",
    "    \n",
    "    if save:\n",
    "        plt.imsave(\"paper_images/lead/original_%s.jpg\" % str(idx),original_img)\n",
    "    \n",
    "    original_img_gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)\n",
    "    root_logits = vgg.root.logits[idx]\n",
    "    case_logits = case_node.logits[idx]\n",
    "    print(\"root logits: \", [np.round(x.item(),2) for x in root_logits])\n",
    "    print(\"fine logits: \", [np.round(x.item(),2) for x in case_logits])\n",
    "    print(\"root softmax: \" ,[np.round(x.item(),6) for x in torch.nn.functional.softmax(root_logits,0)])\n",
    "    print(\"fine softmax: \", [np.round(x.item(),2) for x in torch.nn.functional.softmax(case_logits,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_thresh = .5\n",
    "cmap = \"jet\"\n",
    "\n",
    "for idx in idx_list:\n",
    "    \n",
    "    print(\"STARTING ON ID %.1d \\n \\n\" % idx)\n",
    "    \n",
    "    correct_class = labels[idx]\n",
    "    fine_pred_name = IDfine_names[fine_preds[idx]]\n",
    "    fine_true_name = IDfine_names[labels[idx]]\n",
    "    coarse_pred_name = IDcoarse_names[coarse_preds[idx]]    \n",
    "    coarse_true_name = IDcoarse_names[root_y[idx]]\n",
    "    print(\"\\nlabel: \" + fine_true_name + \", \" + coarse_true_name,\"\\npredictions: \" + fine_pred_name + \", \" + coarse_pred_name)\n",
    "    \n",
    "    original_img = plot_preprocessed_img(images, idx).numpy()\n",
    "    original_img_gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)  \n",
    "    \n",
    "    root_logits = vgg.root.logits[idx]\n",
    "    case_logits = case_node.logits[idx]\n",
    "    \n",
    "    root_max_logit = max(root_logits)\n",
    "    case_max_logit = max(case_logits)\n",
    "    \n",
    "    print(\"\\nBEGIN ROOT ANALYSIS of image %.2d \\n\" % idx)\n",
    "        \n",
    "    array_act, sorted_indices_act = torch.sort(root_activations[idx])\n",
    "    for i in range(1,len(sorted_indices_act)):\n",
    "        print('top {0} activated prototype for this image'.format(i))\n",
    "        print('prototype index: {0}'.format(sorted_indices_act[-i].item()))  \n",
    "        print('prototype class identity: %s' % img_id_to_name(vgg.root.proto_idx[sorted_indices_act[-i]]))                \n",
    "        similarity_score = array_act[-i]\n",
    "        layer_connection = vgg.root_layer.weight[coarse_preds[idx]][sorted_indices_act[-i].item()]\n",
    "        print('activation value (similarity score): {0:.2f}'.format(similarity_score))\n",
    "        print('root layer connection with predicted class: {0:.2f}'.format(layer_connection))\n",
    "        print('contribution to predicted class logit: %.2f' % (similarity_score*layer_connection))\n",
    "        activation_pattern = root_activation_patterns[idx][sorted_indices_act[-i].item()].detach().cpu().numpy()        \n",
    "        upsampled_activation_pattern = resize(activation_pattern, [224,224])\n",
    "        overlayed_img = 0.4 * original_img_gray + 0.6 * upsampled_activation_pattern\n",
    "        #plt.imshow(upsampled_activation_pattern)\n",
    "        #if similarity_score > plot_thresh:\n",
    "        if i <= 4:\n",
    "            plt.imshow(overlayed_img,cmap=cmap)\n",
    "            plt.show()\n",
    "            prototype = show_prototype(model_path + vgg.root.proto_dir, sorted_indices_act[-i].item(),original=True)\n",
    "            if save:\n",
    "                plt.imsave(\"paper_images/lead/overlaid_%s.jpg\" % str(i),overlayed_img,cmap=cmap)\n",
    "                plt.imsave(\"paper_images/lead/top_proto_%s.jpg\" % str(i),prototype)\n",
    "        print('--------------------------------------------------------------')\n",
    "\n",
    "        \n",
    "    print(\"\\nBEGIN FINE ANALYSIS of image %.2d \\n\" % idx)\n",
    "    \n",
    "    case_node = root.get_node(case)\n",
    "    case_children = case_node.children_names()\n",
    "    case_pred_id = 0#case_children.index(fine_pred_name)        \n",
    "    case_layer = getattr(vgg,case+\"_layer\")\n",
    "\n",
    "    array_act, sorted_indices_act = torch.sort(case_activations[idx])\n",
    "    for i in range(1,len(sorted_indices_act)):\n",
    "        print('top {0} activated prototype for this image'.format(i))\n",
    "        print('prototype index: {0}'.format(sorted_indices_act[-i].item()))  \n",
    "        print('prototype class identity: %s' % img_id_to_name(case_node.proto_idx[sorted_indices_act[-i]]))                \n",
    "        similarity_score = array_act[-i]\n",
    "        print('activation value (similarity score): {0:.2f}'.format(similarity_score))\n",
    "        print('case layer connection with predicted class: {0:.2f}'.format(case_layer.weight[case_pred_id][sorted_indices_act[-i].item()]))\n",
    "        print('contribution to predicted class logit: %.2f' % (array_act[-i]*case_layer.weight[case_pred_id][sorted_indices_act[-i].item()]))\n",
    "        activation_pattern = case_activation_patterns[idx][sorted_indices_act[-i].item()].detach().cpu().numpy()        \n",
    "        upsampled_activation_pattern = resize(activation_pattern, [224,224])        \n",
    "        overlayed_img = 0.4 * original_img_gray + 0.6 * upsampled_activation_pattern\n",
    "        #plt.imshow(upsampled_activation_pattern)\n",
    "        #if similarity_score > plot_thresh:\n",
    "        if i <= 4: # (len(sorted_indices_act) - 1):\n",
    "            plt.imshow(overlayed_img,cmap=cmap)\n",
    "            plt.show()\n",
    "            prototype = show_prototype(model_path + case_node.proto_dir, sorted_indices_act[-i].item(),original=True)\n",
    "            if save:\n",
    "                plt.imsave(\"paper_images/lead/fine_overlaid_%s.jpg\" % str(i),overlayed_img,cmap=cmap)\n",
    "                plt.imsave(\"paper_images/lead/fine_top_proto_%s.jpg\" % str(i),prototype)\n",
    "        print('--------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # OOD Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([(x,y) for (x,y) in enumerate(OODfine_names)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forklist is i = 3, q = 3\n",
    "\n",
    "i = 9 # 50 images per class\n",
    "batch_dataset = torch.utils.data.Subset(OOD_dataset, range(i*test_batch_size,(i+1)*test_batch_size))\n",
    "sample_size = 10\n",
    "batch_loader = torch.utils.data.DataLoader(\n",
    "    batch_dataset, batch_size=sample_size, shuffle=False)\n",
    "\n",
    "iter_batch_loader = iter(batch_loader)\n",
    "\n",
    "quintile = 3\n",
    "for i in range(quintile):\n",
    "    images, labels = iter_batch_loader.next()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#x = plot_preprocessed_img(images, 0)\n",
    "\n",
    "for i in range(10):\n",
    "    _ = plot_preprocessed_img(images, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = \"weapon\"\n",
    "\n",
    "eps = vgg.epsilon\n",
    "def activations(min_distances,eps=eps):\n",
    "    return torch.log(1 + (1 / (min_distances + eps)))\n",
    "def activation_patterns(model, x, name, eps=eps):\n",
    "    conv_features = model.conv_features(x)\n",
    "    distances = model.prototype_distances(conv_features,name)\n",
    "    return torch.log(1 + (1 / (distances + eps)))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    images_cuda = images.cuda()\n",
    "    _ = vgg(images_cuda)\n",
    "                    \n",
    "root_y = torch.tensor([OODfineLabel2coarseLabel[y.item()] for y in labels]).cuda()\n",
    "\n",
    "preds_root, preds_joint = vgg.get_joint_distribution()\n",
    "\n",
    "coarse_preds = torch.argmax(preds_root, dim=1)\n",
    "fine_preds = torch.argmax(preds_joint, dim=1)\n",
    "\n",
    "root_activations = activations(vgg.root.min_distances)\n",
    "root_activation_patterns = activation_patterns(vgg, images_cuda, \"root\")\n",
    "case_activations = activations(vgg.root.get_node(case).min_distances)\n",
    "case_activation_patterns = activation_patterns(vgg, images_cuda, case)\n",
    "\n",
    "coarse_correct = (coarse_preds == root_y)\n",
    "            \n",
    "print(\"Batch coarse accuracy: %.2f\" % (coarse_correct.sum().item() / len(labels)))\n",
    "print(\"Indices of correct coarse preds: \", np.where(coarse_correct == 1)[0])\n",
    "print(\"Indices of wrong coarse preds: \", np.where(coarse_correct == 0)[0])\n",
    "\n",
    "del images_cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = [7]\n",
    "import cv2\n",
    "\n",
    "case_node = vgg.root.get_node(case)\n",
    "\n",
    "for idx in idx_list:\n",
    "    fine_pred_name = IDfine_names[fine_preds[idx]]\n",
    "    fine_true_name = OODfine_names[labels[idx]]\n",
    "    coarse_pred_name = OODcoarse_names[coarse_preds[idx]]    \n",
    "    coarse_true_name = OODcoarse_names[root_y[idx]]\n",
    "    print(\"\\nlabel: \" + fine_true_name + \", \" + coarse_true_name,\"\\npredictions: \" + fine_pred_name + \", \" + coarse_pred_name)\n",
    "    original_img = plot_preprocessed_img(images, idx).numpy()\n",
    "    original_img_gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)\n",
    "    root_logits = vgg.root.logits[idx]\n",
    "    case_logits = case_node.logits[idx]\n",
    "    print(\"root logits: \", [np.round(x.item(),2) for x in root_logits])\n",
    "    print(\"%s logits: \" % case_node.name, [np.round(x.item(),2) for x in case_logits])\n",
    "    print(\"root softmax: \" ,[np.round(x.item(),4) for x in torch.nn.functional.softmax(root_logits,0)])\n",
    "    print(\"%s softmax: \" % case_node.name, [np.round(x.item(),2) for x in torch.nn.functional.softmax(case_logits,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_thresh = .5\n",
    "cmap = \"jet\"\n",
    "\n",
    "for idx in idx_list:\n",
    "    \n",
    "    print(\"STARTING ON ID %.1d \\n \\n\" % idx)\n",
    "    \n",
    "    correct_class = labels[idx]\n",
    "    fine_pred_name = IDfine_names[fine_preds[idx]]\n",
    "    fine_true_name = OODfine_names[labels[idx]]\n",
    "    coarse_pred_name = IDcoarse_names[coarse_preds[idx]]    \n",
    "    coarse_true_name = IDcoarse_names[root_y[idx]]\n",
    "    print(\"\\nlabel: \" + fine_true_name + \", \" + coarse_true_name,\"\\npredictions: \" + fine_pred_name + \", \" + coarse_pred_name)\n",
    "    \n",
    "    original_img = plot_preprocessed_img(images, idx).numpy()\n",
    "    original_img_gray = cv2.cvtColor(original_img, cv2.COLOR_BGR2GRAY)    \n",
    "    \n",
    "    if save:\n",
    "        plt.imsave(\"paper_images/case_study/%s%s_original.jpg\" % (case, str(idx)),original_img)   \n",
    "    \n",
    "    print(\"\\nBEGIN ROOT ANALYSIS of image %.2d \\n\" % idx)\n",
    "    \n",
    "    cont_from_top_4 = 0\n",
    "    whole_logit = 0\n",
    "        \n",
    "    array_act, sorted_indices_act = torch.sort(root_activations[idx])\n",
    "    for i in range(1,len(sorted_indices_act)):\n",
    "        print(i)\n",
    "        \n",
    "        #print('prototype class identity: %s' % img_id_to_name(vgg.root.proto_idx[sorted_indices_act[-i]]))                \n",
    "        similarity_score = array_act[-i]\n",
    "        if similarity_score > plot_thresh:\n",
    "            print('top {0} activated prototype for this image'.format(i))\n",
    "            print('prototype index: {0}'.format(sorted_indices_act[-i].item()))      \n",
    "            print('activation value (similarity score): {0:.2f}'.format(similarity_score))\n",
    "            print('root layer connection with predicted class: {0:.2f}'.format(vgg.root_layer.weight[coarse_preds[idx]][sorted_indices_act[-i].item()]))\n",
    "            print('contribution to predicted class logit: %.2f' % (array_act[-i]*vgg.root_layer.weight[coarse_preds[idx]][sorted_indices_act[-i].item()]))\n",
    "        contribution = array_act[-i]*vgg.root_layer.weight[coarse_preds[idx]][sorted_indices_act[-i].item()]        \n",
    "        activation_pattern = root_activation_patterns[idx][sorted_indices_act[-i].item()].detach().cpu().numpy()        \n",
    "        upsampled_activation_pattern = resize(activation_pattern, [224,224])        \n",
    "        overlayed_img = 0.3 * original_img_gray + 0.7 * upsampled_activation_pattern\n",
    "        #plt.imshow(upsampled_activation_pattern)\n",
    "                            \n",
    "        if similarity_score > plot_thresh:            \n",
    "            plt.imshow(overlayed_img,cmap=cmap)\n",
    "            plt.show()\n",
    "            proto = show_prototype(model_path + vgg.root.proto_dir, sorted_indices_act[-i].item(),original=True)\n",
    "            print('--------------------------------------------------------------')\n",
    "            \n",
    "            \n",
    "        whole_logit += contribution\n",
    "        if i <= 4:\n",
    "            cont_from_top_4 += contribution\n",
    "            if save:\n",
    "                plt.imsave(\"paper_images/case_study/%s%s_top%s_proto.jpg\" % (case, str(idx), str(i)),proto)   \n",
    "                plt.imsave(\"paper_images/case_study/%s%s_top%s_heat.jpg\" % (case, str(idx), str(i)),overlayed_img,cmap=cmap)   \n",
    "    \n",
    "            \n",
    "    print(\"cont from top 4: %.2f \\t whole logit: %.2f\" % (cont_from_top_4,whole_logit))\n",
    "        \n",
    "#     print(\"\\nBEGIN FINE ANALYSIS of image %.2d \\n\" % idx)\n",
    "    \n",
    "#     case_node = root.get_node(case)\n",
    "#     case_children = case_node.children_names()\n",
    "#     case_pred_id = case_children.index(fine_pred_name)        \n",
    "#     case_layer = getattr(vgg,case+\"_layer\")\n",
    "\n",
    "#     array_act, sorted_indices_act = torch.sort(case_activations[idx])\n",
    "#     for i in range(1,len(sorted_indices_act)):\n",
    "#         print('top {0} activated prototype for this image'.format(i))\n",
    "#         print('prototype index: {0}'.format(sorted_indices_act[-i].item()))  \n",
    "#         #print('prototype class identity: %s' % img_id_to_name(case_node.proto_idx[sorted_indices_act[-i]]))                \n",
    "#         similarity_score = array_act[-i]\n",
    "#         print('activation value (similarity score): {0:.2f}'.format(similarity_score))\n",
    "#         print('case layer connection with predicted class: {0:.2f}'.format(case_layer.weight[case_pred_id][sorted_indices_act[-i].item()]))\n",
    "#         print('contribution to predicted class logit: %.2f' % (array_act[-i]*case_layer.weight[case_pred_id][sorted_indices_act[-i].item()]))\n",
    "#         activation_pattern = case_activation_patterns[idx][sorted_indices_act[-i].item()].detach().cpu().numpy()        \n",
    "#         upsampled_activation_pattern = resize(activation_pattern, [224,224])        \n",
    "#         overlayed_img = 0.3 * original_img_gray + 0.7 * upsampled_activation_pattern\n",
    "#         #plt.imshow(upsampled_activation_pattern)\n",
    "#         if similarity_score > plot_thresh:\n",
    "#             plt.imshow(overlayed_img)\n",
    "#             plt.show()\n",
    "#             show_prototype(model_path + case_node.proto_dir, sorted_indices_act[-i].item())\n",
    "#         print('--------------------------------------------------------------')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
