{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from helpers import makedir\n",
    "\n",
    "import model\n",
    "import push\n",
    "import train_and_test as tnt\n",
    "from node import Node\n",
    "\n",
    "import save\n",
    "from log import create_logger\n",
    "from preprocess import mean, std, preprocess_input_function, undo_preprocess_input_function\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "data_path = \"../datasets/imagenet/\"\n",
    "model_path = \"saved_models_8protos1/\"\n",
    "resume_path =  model_path + \"best_model_last_opt.pth\"\n",
    "\n",
    "batch_size = 50\n",
    "n_protos_per_class = 8\n",
    "img_size = 224\n",
    "\n",
    "# load the data\n",
    "data_path = data_path\n",
    "train_dir = data_path + 'train/'\n",
    "valid_dir = data_path + 'valid/'\n",
    "test_dir = data_path + 'test/'\n",
    "train_OOD_dir = data_path + 'OOD/train'\n",
    "valid_OOD_dir = data_path + 'OOD/valid'\n",
    "test_OOD_dir = data_path + 'OOD/test'\n",
    "train_push_dir = train_dir\n",
    "train_batch_size = batch_size\n",
    "valid_batch_size = batch_size\n",
    "test_batch_size = batch_size\n",
    "\n",
    "# dataset setup\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "\ttransforms.Resize(256),\n",
    "\ttransforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "])\n",
    "\n",
    "\n",
    "# train set\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    train_dir,\n",
    "    transform_test)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=False)\n",
    "# valid set\n",
    "valid_dataset = datasets.ImageFolder(\n",
    "    valid_dir,\n",
    "    transform_test)\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=valid_batch_size, shuffle=False)    \n",
    "# test set\n",
    "test_dataset = datasets.ImageFolder(\n",
    "    test_dir,\n",
    "    transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "# OOD sets\n",
    "train_OOD_dataset = datasets.ImageFolder(\n",
    "    train_OOD_dir,\n",
    "    transform_test)\n",
    "train_OOD_loader = torch.utils.data.DataLoader(\n",
    "    train_OOD_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "valid_OOD_dataset = datasets.ImageFolder(\n",
    "    valid_OOD_dir,\n",
    "    transform_test)\n",
    "valid_OOD_loader = torch.utils.data.DataLoader(\n",
    "    valid_OOD_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "test_OOD_dataset = datasets.ImageFolder(\n",
    "    test_OOD_dir,\n",
    "    transform_test)\n",
    "test_OOD_loader = torch.utils.data.DataLoader(\n",
    "    test_OOD_dataset, batch_size=valid_batch_size, shuffle=False)\n",
    "\n",
    "print('training set size: {0}'.format(len(train_loader.dataset)))\n",
    "print('valid set size: {0}'.format(len(valid_loader.dataset)))\n",
    "print('test set size: {0}'.format(len(test_loader.dataset)))\t\t\n",
    "print('batch size: {0}'.format(train_batch_size))\n",
    "\n",
    "\n",
    "# construct the tree\n",
    "root = Node(\"root\")\n",
    "root.add_children(['animal','vehicle','everyday_object','weapon','scuba_diver'])\n",
    "root.add_children_to('animal',['non_primate','primate'])\n",
    "root.add_children_to('non_primate',['African_elephant','giant_panda','lion'])\n",
    "root.add_children_to('primate',['capuchin','gibbon','orangutan'])\n",
    "root.add_children_to('vehicle',['ambulance','pickup','sports_car'])\n",
    "root.add_children_to('everyday_object',['laptop','sandal','wine_bottle'])\n",
    "root.add_children_to('weapon',['assault_rifle','rifle'])\n",
    "root.assign_all_descendents()\n",
    "root.assign_proto_dirs()\n",
    "\n",
    "\n",
    "OODroot = Node(\"root\")\n",
    "OODroot.add_children(['animal','vehicle','everyday_object','weapon',\"scuba_diver\"])\n",
    "OODroot.add_children_to('animal',['non_primate','primate'])\n",
    "OODroot.add_children_to('non_primate',['king_penguin','tree_frog','zebra'])\n",
    "OODroot.add_children_to('primate',['macaque','gorilla','chimpanzee'])\n",
    "OODroot.add_children_to('vehicle',['cab','forklift','tractor','mountain_bike'])\n",
    "OODroot.add_children_to('everyday_object',['golf_ball','wallet','table_lamp'])\n",
    "OODroot.add_children_to('weapon',['revolver','bow'])\n",
    "OODroot.assign_all_descendents()\n",
    "OODroot.assign_proto_dirs()\n",
    "\n",
    "\n",
    "IDcoarse_names = root.children_names()\n",
    "IDfine_names = os.listdir(train_dir)\n",
    "IDfine_names.sort()\n",
    "label2name = {i : name for (i,name) in enumerate(IDfine_names)}\n",
    "IDfineLabel2coarseLabel = {label : root.children_to_labels[root.closest_descendent_for(name).name] for label, name in enumerate(IDfine_names)}    \n",
    "\n",
    "OODcoarse_names = OODroot.children_names()\n",
    "OODfine_names = os.listdir(train_OOD_dir)\n",
    "OODfine_names.sort()\n",
    "OODlabel2name = {i : name for (i,name) in enumerate(OODfine_names)}\n",
    "OODfineLabel2coarseLabel = {label : OODroot.children_to_labels[OODroot.closest_descendent_for(name).name] for label, name in enumerate(OODfine_names)}\n",
    "\n",
    "num_fine = len(IDfine_names)\n",
    "num_coarse = len(root.children)\n",
    "\n",
    "\n",
    "vgg = model.vgg16_proto(root, pretrained=True, num_prototypes_per_class=n_protos_per_class,\n",
    "                        prototype_dimension = 32, img_size=img_size, resume_path = resume_path)\n",
    "vgg = vgg.cuda()\n",
    "vgg_multi = torch.nn.DataParallel(vgg)\n",
    "class_specific=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_acc = tnt.test(model=vgg_multi, dataloader=valid_loader, label2name=label2name, class_specific=class_specific, log=print)\n",
    "\n",
    "#test_acc = tnt.test(model=vgg_multi, dataloader=test_loader, label2name=label2name, class_specific=class_specific, log=print)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get logits for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_X(model, dataloader, label2name, OOD = False, log=print):\n",
    "    \n",
    "    for node in model.module.root.nodes_with_children():\n",
    "        setattr(node,\"logits_all\", np.ndarray(shape=(0,node.num_children())))\n",
    "    \n",
    "    coarse_names = model.module.root.children_names()\n",
    "    num_coarse = len(coarse_names)\n",
    "\n",
    "    fine_names = [x for x in label2name.values()]\n",
    "    num_fine = len(fine_names)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i, (image, label) in enumerate(dataloader):\n",
    "        input = image.cuda()\n",
    "        target = label.cuda()\n",
    "\n",
    "        batch_names = [label2name[y.item()] for y in label]     \n",
    "        batch_size = len(target)\n",
    "\n",
    "        batch_start = time.time()   \n",
    "\n",
    "        cross_entropy = 0\n",
    "        cluster_cost = 0\n",
    "        separation_cost = 0        \n",
    "        l1 = 0\n",
    "        noise_cross_ent = 0\n",
    "        \n",
    "        #print(batch_names)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            _ = model(input) \n",
    "\n",
    "            for node in model.module.root.nodes_with_children():\n",
    "                # get names specific to children\n",
    "                if not OOD:\n",
    "                    children_idx = torch.tensor([name in node.descendents for name in batch_names])\n",
    "                    batch_names_coarsest = [node.closest_descendent_for(name).name for name in batch_names if name in node.descendents] # size of sum(children_idx)\n",
    "                    node_y = torch.tensor([node.children_to_labels[name] for name in batch_names_coarsest]).cuda() # size of sum(children_idx)\n",
    "                elif OOD:\n",
    "                    OODnode = OODroot.get_node(node.name)\n",
    "                    children_idx = children_idx = torch.tensor([name in OODnode.descendents for name in batch_names])\n",
    "                    batch_names_coarsest = [OODnode.closest_descendent_for(name).name for name in batch_names if name in OODnode.descendents] # size of sum(children_idx)\n",
    "                    node_y = torch.tensor([OODnode.children_to_labels[name] for name in batch_names_coarsest]).cuda() # size of sum(children_idx)\n",
    "                    #print(batch_names_coarsest)\n",
    "                \n",
    "                node_logits = node.logits[children_idx]            \n",
    "                node_logits = node_logits.detach().cpu().numpy()\n",
    "                \n",
    "                if len(node_y) > 0:\n",
    "                    node.logits_all = np.concatenate((node.logits_all,node_logits))\n",
    "                                        \n",
    "        del input\n",
    "        del target, node_y\n",
    "        \n",
    "        batch_end = time.time()\n",
    "        \n",
    "\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    log('time: {0:.2f}'.format(end -  start))\n",
    "\n",
    "    return None   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_X(vgg_multi, valid_loader, label2name)\n",
    "\n",
    "for node in vgg.root.nodes_with_children():\n",
    "    print(node.name, node.logits_all.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train\n",
    "IDtrain_subset : OOD1250classes1\n",
    "\n",
    "valid\n",
    "IDvalid_subset : OOD50classes1\n",
    "\n",
    "test\n",
    "IDtest : OOD50classes2\n",
    "\n",
    "For each of the furthest parents, cycle through possible splits at each set of lowest parents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "furthest_parents = ['non_primate','primate','vehicle','everday_object','weapon']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "# for balanced data, need to subset the ID data each time, since when fitting the OOD classes get split between train and test\n",
    "balance1_idx = np.random.choice(np.arange(3750),2500,replace=False)\n",
    "balance2_idx = np.random.choice(np.arange(2500),1250,replace=False)\n",
    "\n",
    "# train data\n",
    "_ = get_X(vgg_multi, train_loader, label2name)\n",
    "\n",
    "train_np_X = vgg.root.get_node(\"non_primate\").logits_all[balance1_idx]\n",
    "\n",
    "train_p_X = vgg.root.get_node(\"primate\").logits_all[balance1_idx]\n",
    "\n",
    "train_v_X = vgg.root.get_node(\"vehicle\").logits_all[balance1_idx]\n",
    "\n",
    "train_e_X = vgg.root.get_node(\"everyday_object\").logits_all[balance1_idx]\n",
    "\n",
    "train_w_X = vgg.root.get_node(\"weapon\").logits_all[balance2_idx]\n",
    "\n",
    "print(train_np_X.shape)\n",
    "print(train_w_X.shape)\n",
    "\n",
    "balance1_idx = np.random.choice(np.arange(150),100,replace=False)\n",
    "balance2_idx = np.random.choice(np.arange(100),50,replace=False)\n",
    "\n",
    "# valid data\n",
    "_ = get_X(vgg_multi, valid_loader, label2name)\n",
    "\n",
    "valid_np_X = vgg.root.get_node(\"non_primate\").logits_all[balance1_idx]\n",
    "\n",
    "valid_p_X = vgg.root.get_node(\"primate\").logits_all[balance1_idx]\n",
    "\n",
    "valid_v_X = vgg.root.get_node(\"vehicle\").logits_all[balance1_idx]\n",
    "\n",
    "valid_e_X = vgg.root.get_node(\"everyday_object\").logits_all[balance1_idx]\n",
    "\n",
    "valid_w_X = vgg.root.get_node(\"weapon\").logits_all[balance2_idx]\n",
    "\n",
    "print(valid_np_X.shape)\n",
    "print(valid_w_X.shape)\n",
    "\n",
    "balance1_idx = np.random.choice(np.arange(150),50,replace=False)\n",
    "balance2_idx = np.random.choice(np.arange(100),50,replace=False)\n",
    "\n",
    "# test data\n",
    "_ = get_X(vgg_multi, test_loader, label2name)\n",
    "\n",
    "test_np_X = vgg.root.get_node(\"non_primate\").logits_all[balance1_idx]\n",
    "\n",
    "test_p_X = vgg.root.get_node(\"primate\").logits_all[balance1_idx]\n",
    "\n",
    "test_v_X = vgg.root.get_node(\"vehicle\").logits_all[balance1_idx]\n",
    "\n",
    "test_e_X = vgg.root.get_node(\"everyday_object\").logits_all[balance1_idx]\n",
    "\n",
    "test_w_X = vgg.root.get_node(\"weapon\").logits_all[balance2_idx]\n",
    "\n",
    "print(test_np_X.shape)\n",
    "print(test_w_X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make train\n",
    "_ = get_X(vgg_multi, train_OOD_loader, OODlabel2name,  OOD = True)\n",
    "\n",
    "train_np_logits = vgg.root.get_node(\"non_primate\").logits_all\n",
    "train_p_logits = vgg.root.get_node(\"primate\").logits_all\n",
    "train_v_logits = vgg.root.get_node(\"vehicle\").logits_all\n",
    "train_e_logits = vgg.root.get_node(\"everyday_object\").logits_all\n",
    "train_w_logits = vgg.root.get_node(\"weapon\").logits_all\n",
    "\n",
    "print(train_np_logits.shape)\n",
    "print(train_w_logits.shape)\n",
    "\n",
    "# make valid\n",
    "_ = get_X(vgg_multi, valid_OOD_loader, OODlabel2name, OOD = True)\n",
    "\n",
    "valid_np_logits = vgg.root.get_node(\"non_primate\").logits_all\n",
    "valid_p_logits = vgg.root.get_node(\"primate\").logits_all\n",
    "valid_v_logits = vgg.root.get_node(\"vehicle\").logits_all\n",
    "valid_e_logits = vgg.root.get_node(\"everyday_object\").logits_all\n",
    "valid_w_logits = vgg.root.get_node(\"weapon\").logits_all\n",
    "\n",
    "print(valid_np_logits.shape)\n",
    "print(valid_w_logits.shape)\n",
    "\n",
    "# make test\n",
    "_ = get_X(vgg_multi, test_OOD_loader, OODlabel2name, OOD = True)\n",
    "\n",
    "test_np_logits = vgg.root.get_node(\"non_primate\").logits_all\n",
    "test_p_logits = vgg.root.get_node(\"primate\").logits_all\n",
    "test_v_logits = vgg.root.get_node(\"vehicle\").logits_all\n",
    "test_e_logits = vgg.root.get_node(\"everyday_object\").logits_all\n",
    "test_w_logits = vgg.root.get_node(\"weapon\").logits_all\n",
    "\n",
    "print(test_np_logits.shape)\n",
    "print(test_w_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = 1\n",
    "test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "train_valid_idx = np.setdiff1d(np.arange(3750),test_idx)\n",
    "\n",
    "# train\n",
    "\n",
    "full_train_np_X = np.concatenate((train_np_X,train_np_logits[train_valid_idx]))\n",
    "full_train_np_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "full_train_p_X = np.concatenate((train_p_X,train_p_logits[train_valid_idx]))\n",
    "full_train_p_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "full_train_v_X = np.concatenate((train_v_X,train_v_logits[train_valid_idx]))\n",
    "full_train_v_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "full_train_e_X = np.concatenate((train_e_X,train_e_logits[train_valid_idx]))\n",
    "full_train_e_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "train_valid_idx = np.setdiff1d(np.arange(2500),test_idx)\n",
    "full_train_w_X = np.concatenate((train_w_X,train_w_logits[train_valid_idx]))\n",
    "full_train_w_Y = np.concatenate((np.zeros(1250),np.ones(1250)))\n",
    "\n",
    "# valid\n",
    "\n",
    "test_idx = np.arange(50*trial,50*(trial+1))\n",
    "train_valid_idx = np.setdiff1d(np.arange(150),test_idx)\n",
    "\n",
    "\n",
    "full_valid_np_X = np.concatenate((valid_np_X,valid_np_logits[train_valid_idx]))\n",
    "full_valid_np_Y = np.concatenate((np.zeros(100),np.ones(100)))\n",
    "\n",
    "full_valid_p_X = np.concatenate((valid_p_X,valid_p_logits[train_valid_idx]))\n",
    "full_valid_p_Y = np.concatenate((np.zeros(100),np.ones(100)))\n",
    "\n",
    "full_valid_v_X = np.concatenate((valid_v_X,valid_v_logits[train_valid_idx]))\n",
    "full_valid_v_Y = np.concatenate((np.zeros(100),np.ones(100)))\n",
    "\n",
    "full_valid_e_X = np.concatenate((valid_e_X,valid_e_logits[train_valid_idx]))\n",
    "full_valid_e_Y = np.concatenate((np.zeros(100),np.ones(100)))\n",
    "\n",
    "train_valid_idx = np.setdiff1d(np.arange(100),test_idx)\n",
    "full_valid_w_X = np.concatenate((valid_w_X,valid_w_logits[train_valid_idx]))\n",
    "full_valid_w_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "# test\n",
    "\n",
    "full_test_np_X = np.concatenate((test_np_X,test_np_logits[test_idx]))\n",
    "full_test_np_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "full_test_p_X = np.concatenate((test_p_X,test_p_logits[test_idx]))\n",
    "full_test_p_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "full_test_v_X = np.concatenate((test_v_X,test_v_logits[test_idx]))\n",
    "full_test_v_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "full_test_e_X = np.concatenate((test_e_X,test_e_logits[test_idx]))\n",
    "full_test_e_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "full_test_w_X = np.concatenate((test_w_X,test_w_logits[test_idx]))\n",
    "full_test_w_Y = np.concatenate((np.zeros(50),np.ones(50)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_thresh(thresh,X,Y):\n",
    "    Yhat = 1 * (X > thresh)\n",
    "    return np.mean(Yhat == Y)\n",
    "\n",
    "def line_search(X,Y,probs=False):        \n",
    "    thresholds = np.arange(np.min(X),np.max(X), (np.max(X) - np.min(X)) / 150)\n",
    "    thresh2acc = {}\n",
    "    for thresh in thresholds:\n",
    "        thresh2acc[thresh] = score_thresh(thresh,X,Y)\n",
    "    return np.array([x for x in thresh2acc.items()])\n",
    "\n",
    "def full_line_search(train_X,train_Y,test_X,test_Y,probs=False):\n",
    "    if probs:\n",
    "        train_Xprime = torch.nn.functional.softmax(torch.tensor(train_X),1).numpy()\n",
    "        test_Xprime = torch.nn.functional.softmax(torch.tensor(test_X),1).numpy()\n",
    "        train_Xprime = np.max(train_Xprime,1)\n",
    "        test_Xprime = np.max(test_Xprime,1)\n",
    "    else:\n",
    "        train_Xprime = np.max(train_X,1)\n",
    "        test_Xprime = np.max(test_X,1)    \n",
    "    \n",
    "    train = line_search(train_Xprime,train_Y,probs=probs)\n",
    "    \n",
    "    best_id = np.argmax(train[:,1])\n",
    "    best_thresh = train[:,0][best_id]    \n",
    "    \n",
    "    train_acc = score_thresh(best_thresh,train_Xprime,train_Y)\n",
    "    test_acc = score_thresh(best_thresh,test_Xprime,test_Y)\n",
    "    \n",
    "    \n",
    "    return best_thresh, test_acc, test_acc\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def lr_line_search(X,Y,probs=False):\n",
    "    \n",
    "    model = LogisticRegression(C=1e6)            \n",
    "    model.fit(X,Y)    \n",
    "    \n",
    "    return model\n",
    "    \n",
    "def full_lr_line_search(train_X,train_Y,test_X,test_Y,probs=False):\n",
    "    if probs:\n",
    "        train_Xprime = torch.nn.functional.softmax(torch.tensor(train_X),1).numpy()\n",
    "        test_Xprime = torch.nn.functional.softmax(torch.tensor(test_X),1).numpy()\n",
    "        #train_Xprime = np.max(train_Xprime,1).reshape(-1,1)\n",
    "        #test_Xprime = np.max(test_Xprime,1).reshape(-1,1)\n",
    "    else:\n",
    "        train_Xprime = train_X#np.max(train_X,1).reshape(-1,1)#train_X \n",
    "        test_Xprime = test_X#np.max(test_X,1).reshape(-1,1) #test_X \n",
    "    \n",
    "    model = lr_line_search(train_Xprime,train_Y,probs=probs)\n",
    "            \n",
    "    train_acc = model.score(train_Xprime,train_Y)\n",
    "    test_acc = model.score(test_Xprime,test_Y)    \n",
    "    \n",
    "    return 0, train_acc, test_acc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non primates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "gs = [1] # using linear kernel now\n",
    "cs = [.001,.01,.1,.5,1,10]\n",
    "\n",
    "node = OODroot.get_node(\"non_primate\")\n",
    "print(\"OOD non primates: \", node.children_names())\n",
    "\n",
    "trial_accs = np.zeros(node.num_children())\n",
    "\n",
    "logit_thresh_accs = np.zeros(node.num_children())\n",
    "probs_thresh_accs = np.zeros(node.num_children())\n",
    "\n",
    "for trial in range(node.num_children()):\n",
    "    \n",
    "    print(\"\\ntrial: \", trial)\n",
    "    print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "    \n",
    "    train_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    valid_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    \n",
    "    # get data\n",
    "    test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "    train_valid_idx = np.setdiff1d(np.arange(3750),test_idx)\n",
    "    \n",
    "    full_train_np_X = np.concatenate((train_np_X,train_np_logits[train_valid_idx]))\n",
    "    full_train_np_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "    test_idx = np.arange(50*trial,50*(trial+1))\n",
    "    train_valid_idx = np.setdiff1d(np.arange(150),test_idx)\n",
    "\n",
    "    full_valid_np_X = np.concatenate((valid_np_X,valid_np_logits[train_valid_idx]))\n",
    "    full_valid_np_Y = np.concatenate((np.zeros(100),np.ones(100)))    \n",
    "    \n",
    "    full_test_np_X = np.concatenate((test_np_X,test_np_logits[test_idx]))\n",
    "    full_test_np_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "    \n",
    "    \n",
    "    # thresholding\n",
    "    logit_thresh, _, logit_thresh_acc = full_lr_line_search(full_train_np_X,full_train_np_Y,\n",
    "                                                      full_test_np_X,full_test_np_Y,probs=False)  \n",
    "    \n",
    "    logit_thresh_accs[trial] = logit_thresh_acc\n",
    "    \n",
    "    probs_thresh, _, probs_thresh_acc = full_lr_line_search(full_train_np_X,full_train_np_Y,\n",
    "                                                      full_test_np_X,full_test_np_Y,probs=True)    \n",
    "    probs_thresh_accs[trial] = probs_thresh_acc\n",
    "    \n",
    "    \n",
    "    # SVM cross validation fitting\n",
    "    for g in range(len(gs)):\n",
    "        for c in range(len(cs)): \n",
    "\n",
    "#             print(\"iter: %d / %d \\t starting g: %.3f and c: %.3f\" % (g*len(cs) + c, len(gs) * len(cs), gs[g],cs[c]))\n",
    "\n",
    "            SVM = SVC(kernel='linear',gamma=gs[g],C=cs[c])\n",
    "            SVM.fit(full_train_np_X,full_train_np_Y)\n",
    "\n",
    "            yhat = SVM.predict(full_train_np_X)\n",
    "            ytrue = full_train_np_Y\n",
    "            conf_mat = confusion_matrix(ytrue,yhat)\n",
    "\n",
    "            train_acc = SVM.score(full_train_np_X,full_train_np_Y)\n",
    "            valid_acc = SVM.score(full_valid_np_X,full_valid_np_Y)\n",
    "\n",
    "            train_accs[g,c] = train_acc\n",
    "            valid_accs[g,c] = valid_acc\n",
    "\n",
    "    row_id = valid_accs.argmax() // len(cs)\n",
    "    col_id = valid_accs.argmax() - (row_id+1)*len(cs)\n",
    "    best_g = gs[row_id]\n",
    "    best_c = cs[col_id]\n",
    "    \n",
    "    SVM = SVC(kernel='linear',gamma=best_g,C=best_c)\n",
    "    SVM.fit(full_train_np_X,full_train_np_Y)\n",
    "\n",
    "    test_acc = SVM.score(full_test_np_X,full_test_np_Y)\n",
    "    \n",
    "    trial_accs[trial] = test_acc\n",
    "    \n",
    "    print(\"\\n valid accs \\n\")\n",
    "    print(valid_accs)\n",
    "    \n",
    "    print(\"best g: \", best_g)\n",
    "    print(\"best c: \", best_c)\n",
    "        \n",
    "    print(\"test acc: \",test_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM accs\")\n",
    "print(trial_accs)\n",
    "print(\"mean acc: \",np.mean(trial_accs))\n",
    "np_svm_mean = np.mean(trial_accs)\n",
    "\n",
    "print(\"\\nlogit thresh accs\")\n",
    "print(logit_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(logit_thresh_accs))\n",
    "np_logit_mean = np.mean(logit_thresh_accs)\n",
    "\n",
    "print(\"\\nprobs thresh accs\")\n",
    "print(probs_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(probs_thresh_accs))\n",
    "np_prob_mean = np.mean(probs_thresh_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## primates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "gs = [1] # using linear kernel now\n",
    "cs = [.001,.01,.1,.5,1,10]\n",
    "\n",
    "\n",
    "node = OODroot.get_node(\"primate\")\n",
    "print(\"OOD primates: \", node.children_names())\n",
    "\n",
    "trial_accs = np.zeros(node.num_children())\n",
    "\n",
    "logit_thresh_accs = np.zeros(node.num_children())\n",
    "probs_thresh_accs = np.zeros(node.num_children())\n",
    "\n",
    "for trial in range(node.num_children()):\n",
    "    \n",
    "    print(\"\\ntrial: \", trial)\n",
    "    print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "    \n",
    "    train_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    valid_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    \n",
    "    # get data\n",
    "    test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "    train_valid_idx = np.setdiff1d(np.arange(3750),test_idx)\n",
    "    \n",
    "    full_train_p_X = np.concatenate((train_p_X,train_p_logits[train_valid_idx]))\n",
    "    full_train_p_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "    test_idx = np.arange(50*trial,50*(trial+1))\n",
    "    train_valid_idx = np.setdiff1d(np.arange(150),test_idx)\n",
    "\n",
    "    full_valid_p_X = np.concatenate((valid_p_X,valid_p_logits[train_valid_idx]))\n",
    "    full_valid_p_Y = np.concatenate((np.zeros(100),np.ones(100)))  \n",
    "    \n",
    "    full_test_p_X = np.concatenate((test_p_X,test_p_logits[test_idx]))\n",
    "    full_test_p_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "            \n",
    "    # thresholding\n",
    "    logit_thresh, _, logit_thresh_acc = full_lr_line_search(full_train_p_X,full_train_p_Y,\n",
    "                                                      full_test_p_X,full_test_p_Y,probs=False)  \n",
    "    \n",
    "    logit_thresh_accs[trial] = logit_thresh_acc\n",
    "    \n",
    "    probs_thresh, _, probs_thresh_acc = full_lr_line_search(full_train_p_X,full_train_p_Y,\n",
    "                                                      full_test_p_X,full_test_p_Y,probs=True)    \n",
    "    probs_thresh_accs[trial] = probs_thresh_acc\n",
    "    \n",
    "    \n",
    "    # SVM cross validation fitting\n",
    "    for g in range(len(gs)):\n",
    "        for c in range(len(cs)): \n",
    "\n",
    "#             print(\"iter: %d / %d \\t starting g: %.3f and c: %.3f\" % (g*len(cs) + c, len(gs) * len(cs), gs[g],cs[c]))\n",
    "\n",
    "            SVM = SVC(kernel='linear',gamma=gs[g],C=cs[c])\n",
    "            SVM.fit(full_train_p_X,full_train_p_Y)\n",
    "\n",
    "            yhat = SVM.predict(full_train_p_X)\n",
    "            ytrue = full_train_p_Y\n",
    "            conf_mat = confusion_matrix(ytrue,yhat)\n",
    "\n",
    "    #         print(\"\\nconf mat \\n\")\n",
    "    #         print(np.stack([[np.round(x/np.sum(row),2) for x in row] for row in conf_mat]))        \n",
    "\n",
    "            train_acc = SVM.score(full_train_p_X,full_train_p_Y)\n",
    "            valid_acc = SVM.score(full_valid_p_X,full_valid_p_Y)\n",
    "\n",
    "    #         print(\"train acc: %.2f \" % train_acc)\n",
    "    #         print(\"valid acc: %.2f \" % valid_acc)\n",
    "\n",
    "            train_accs[g,c] = train_acc\n",
    "            valid_accs[g,c] = valid_acc\n",
    "\n",
    "    row_id = valid_accs.argmax() // len(cs)\n",
    "    col_id = valid_accs.argmax() - (row_id+1)*len(cs)\n",
    "    best_g = gs[row_id]\n",
    "    best_c = cs[col_id]\n",
    "    \n",
    "    SVM = SVC(kernel='linear',gamma=best_g,C=best_c)\n",
    "    SVM.fit(full_train_p_X,full_train_p_Y)\n",
    "\n",
    "    test_acc = SVM.score(full_test_p_X,full_test_p_Y)\n",
    "    \n",
    "    trial_accs[trial] = test_acc\n",
    "    \n",
    "    print(\"\\n valid accs \\n\")\n",
    "    print(valid_accs)\n",
    "    \n",
    "    print(\"best g: \", best_g)\n",
    "    print(\"best c: \", best_c)\n",
    "        \n",
    "    print(\"test acc: \",test_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM accs\")\n",
    "print(trial_accs)\n",
    "print(\"mean acc: \",np.mean(trial_accs))\n",
    "p_svm_mean = np.mean(trial_accs)\n",
    "\n",
    "print(\"\\nlogit thresh accs\")\n",
    "print(logit_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(logit_thresh_accs))\n",
    "p_logit_mean = np.mean(logit_thresh_accs)\n",
    "\n",
    "print(\"\\nprobs thresh accs\")\n",
    "print(probs_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(probs_thresh_accs))\n",
    "p_prob_mean = np.mean(probs_thresh_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vehicles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "gs = [1] # using linear kernel now\n",
    "cs = [.001,.01,.1,.5,1,10]\n",
    "\n",
    "node = OODroot.get_node(\"vehicle\")\n",
    "print(\"OOD vehicles: \", node.children_names())\n",
    "\n",
    "trial_accs = np.zeros(node.num_children())\n",
    "\n",
    "logit_thresh_accs = np.zeros(node.num_children())\n",
    "probs_thresh_accs = np.zeros(node.num_children())\n",
    "\n",
    "\n",
    "for trial in range(node.num_children()):\n",
    "    \n",
    "    print(\"\\ntrial: \", trial)\n",
    "    print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "    \n",
    "    train_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    valid_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    \n",
    "    # get data\n",
    "    test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "    train_valid_idx = np.setdiff1d(np.arange(5000),test_idx)\n",
    "    \n",
    "    full_train_v_X = np.concatenate((train_v_X,train_v_logits[train_valid_idx]))\n",
    "    full_train_v_Y = np.concatenate((np.zeros(2500),np.ones(3750))) # data was not balanced here\n",
    "\n",
    "    print(trial)\n",
    "    test_idx = np.arange(50*trial,50*(trial+1))\n",
    "    train_valid_idx = np.setdiff1d(np.arange(200),test_idx)\n",
    "\n",
    "    full_valid_v_X = np.concatenate((valid_v_X,valid_v_logits[train_valid_idx]))\n",
    "    full_valid_v_Y = np.concatenate((np.zeros(100),np.ones(150)))    \n",
    "\n",
    "    full_test_v_X = np.concatenate((test_v_X,test_v_logits[test_idx]))\n",
    "    full_test_v_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "    \n",
    "    # thresholding\n",
    "    logit_thresh, _, logit_thresh_acc = full_lr_line_search(full_train_v_X,full_train_v_Y,\n",
    "                                                      full_test_v_X,full_test_v_Y,probs=False)  \n",
    "    \n",
    "    logit_thresh_accs[trial] = logit_thresh_acc\n",
    "    \n",
    "    probs_thresh, _, probs_thresh_acc = full_lr_line_search(full_train_v_X,full_train_v_Y,\n",
    "                                                      full_test_v_X,full_test_v_Y,probs=True)    \n",
    "    probs_thresh_accs[trial] = probs_thresh_acc\n",
    "    \n",
    "    \n",
    "    # SVM cross validation fitting\n",
    "    for g in range(len(gs)):\n",
    "        for c in range(len(cs)): \n",
    "\n",
    "#             print(\"iter: %d / %d \\t starting g: %.3f and c: %.3f\" % (g*len(cs) + c, len(gs) * len(cs), gs[g],cs[c]))\n",
    "\n",
    "            SVM = SVC(kernel='linear',gamma=gs[g],C=cs[c])\n",
    "            SVM.fit(full_train_v_X,full_train_v_Y)\n",
    "\n",
    "            yhat = SVM.predict(full_train_v_X)\n",
    "            ytrue = full_train_v_Y\n",
    "            conf_mat = confusion_matrix(ytrue,yhat)\n",
    "\n",
    "    #         print(\"\\nconf mat \\n\")\n",
    "    #         print(np.stack([[np.round(x/np.sum(row),2) for x in row] for row in conf_mat]))        \n",
    "\n",
    "            train_acc = SVM.score(full_train_v_X,full_train_v_Y)\n",
    "            valid_acc = SVM.score(full_valid_v_X,full_valid_v_Y)\n",
    "\n",
    "    #         print(\"train acc: %.2f \" % train_acc)\n",
    "    #         print(\"valid acc: %.2f \" % valid_acc)\n",
    "\n",
    "            train_accs[g,c] = train_acc\n",
    "            valid_accs[g,c] = valid_acc\n",
    "\n",
    "    row_id = valid_accs.argmax() // len(cs)\n",
    "    col_id = valid_accs.argmax() - (row_id+1)*len(cs)\n",
    "    best_g = gs[row_id]\n",
    "    best_c = cs[col_id]\n",
    "    \n",
    "    SVM = SVC(kernel='linear',gamma=best_g,C=best_c)\n",
    "    SVM.fit(full_train_v_X,full_train_v_Y)\n",
    "\n",
    "    test_acc = SVM.score(full_test_v_X,full_test_v_Y)\n",
    "    \n",
    "    trial_accs[trial] = test_acc\n",
    "    \n",
    "    print(\"\\n valid accs \\n\")\n",
    "    print(valid_accs)\n",
    "    \n",
    "    print(\"best g: \", best_g)\n",
    "    print(\"best c: \", best_c)\n",
    "        \n",
    "    print(\"test acc: \",test_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM accs\")\n",
    "print(trial_accs)\n",
    "print(\"mean acc: \",np.mean(trial_accs))\n",
    "v_svm_mean = np.mean(trial_accs)\n",
    "\n",
    "print(\"\\nlogit thresh accs\")\n",
    "print(logit_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(logit_thresh_accs))\n",
    "v_logit_mean = np.mean(logit_thresh_accs)\n",
    "\n",
    "print(\"\\nprobs thresh accs\")\n",
    "print(probs_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(probs_thresh_accs))\n",
    "v_prob_mean = np.mean(probs_thresh_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## everyday object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "gs = [1] # using linear kernel now\n",
    "cs = [.001,.01,.1,.5,1,10]\n",
    "\n",
    "node = OODroot.get_node(\"everyday_object\")\n",
    "print(\"OOD everyday objects: \", node.children_names())\n",
    "\n",
    "trial_accs = np.zeros(node.num_children())\n",
    "\n",
    "logit_thresh_accs = np.zeros(node.num_children())\n",
    "probs_thresh_accs = np.zeros(node.num_children())\n",
    "\n",
    "\n",
    "for trial in range(node.num_children()):\n",
    "    \n",
    "    print(\"\\ntrial: \", trial)\n",
    "    print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "    \n",
    "    train_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    valid_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    \n",
    "    # get data\n",
    "    test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "    train_valid_idx = np.setdiff1d(np.arange(3750),test_idx)\n",
    "    \n",
    "    full_train_e_X = np.concatenate((train_e_X,train_e_logits[train_valid_idx]))\n",
    "    full_train_e_Y = np.concatenate((np.zeros(2500),np.ones(2500)))\n",
    "\n",
    "    test_idx = np.arange(50*trial,50*(trial+1))\n",
    "    train_valid_idx = np.setdiff1d(np.arange(150),test_idx)\n",
    "\n",
    "    full_valid_e_X = np.concatenate((valid_e_X,valid_e_logits[train_valid_idx]))\n",
    "    full_valid_e_Y = np.concatenate((np.zeros(100),np.ones(100)))    \n",
    "    \n",
    "    full_test_e_X = np.concatenate((test_e_X,test_e_logits[test_idx]))\n",
    "    full_test_e_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "    \n",
    "    # thresholding\n",
    "    logit_thresh, _, logit_thresh_acc = full_lr_line_search(full_train_e_X,full_train_e_Y,\n",
    "                                                      full_test_e_X,full_test_e_Y,probs=False)  \n",
    "    \n",
    "    logit_thresh_accs[trial] = logit_thresh_acc\n",
    "    \n",
    "    probs_thresh, _, probs_thresh_acc = full_lr_line_search(full_train_e_X,full_train_e_Y,\n",
    "                                                      full_test_e_X,full_test_e_Y,probs=True)    \n",
    "    probs_thresh_accs[trial] = probs_thresh_acc\n",
    "    \n",
    "    \n",
    "    # SVM cross validation fitting\n",
    "    for g in range(len(gs)):\n",
    "        for c in range(len(cs)): \n",
    "\n",
    "#             print(\"iter: %d / %d \\t starting g: %.3f and c: %.3f\" % (g*len(cs) + c, len(gs) * len(cs), gs[g],cs[c]))\n",
    "\n",
    "            SVM = SVC(kernel='linear',gamma=gs[g],C=cs[c])\n",
    "            SVM.fit(full_train_e_X,full_train_e_Y)\n",
    "\n",
    "            yhat = SVM.predict(full_train_e_X)\n",
    "            ytrue = full_train_e_Y\n",
    "            conf_mat = confusion_matrix(ytrue,yhat)\n",
    "\n",
    "    #         print(\"\\nconf mat \\n\")\n",
    "    #         print(np.stack([[np.round(x/np.sum(row),2) for x in row] for row in conf_mat]))        \n",
    "\n",
    "            train_acc = SVM.score(full_train_e_X,full_train_e_Y)\n",
    "            valid_acc = SVM.score(full_valid_e_X,full_valid_e_Y)\n",
    "\n",
    "    #         print(\"train acc: %.2f \" % train_acc)\n",
    "    #         print(\"valid acc: %.2f \" % valid_acc)\n",
    "\n",
    "            train_accs[g,c] = train_acc\n",
    "            valid_accs[g,c] = valid_acc\n",
    "\n",
    "    row_id = valid_accs.argmax() // len(cs)\n",
    "    col_id = valid_accs.argmax() - (row_id+1)*len(cs)\n",
    "    best_g = gs[row_id]\n",
    "    best_c = cs[col_id]\n",
    "    \n",
    "    SVM = SVC(kernel='linear',gamma=best_g,C=best_c)\n",
    "    SVM.fit(full_train_e_X,full_train_e_Y)\n",
    "\n",
    "    test_acc = SVM.score(full_test_e_X,full_test_e_Y)\n",
    "    \n",
    "    trial_accs[trial] = test_acc\n",
    "    \n",
    "    print(\"\\n valid accs \\n\")\n",
    "    print(valid_accs)\n",
    "    \n",
    "    print(\"best g: \", best_g)\n",
    "    print(\"best c: \", best_c)\n",
    "        \n",
    "    print(\"test acc: \",test_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM accs\")\n",
    "print(trial_accs)\n",
    "print(\"mean acc: \",np.mean(trial_accs))\n",
    "e_svm_mean = np.mean(trial_accs)\n",
    "\n",
    "print(\"\\nlogit thresh accs\")\n",
    "print(logit_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(logit_thresh_accs))\n",
    "e_logit_mean = np.mean(logit_thresh_accs)\n",
    "\n",
    "print(\"\\nprobs thresh accs\")\n",
    "print(probs_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(probs_thresh_accs))\n",
    "e_prob_mean = np.mean(probs_thresh_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weapon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "\n",
    "gs = [1] # using linear kernel now\n",
    "cs = [.001,.01,.1,.5,1,10]\n",
    "\n",
    "node = OODroot.get_node(\"weapon\")\n",
    "print(\"OOD weapons: \", node.children_names())\n",
    "\n",
    "trial_accs = np.zeros(node.num_children())\n",
    "\n",
    "logit_thresh_accs = np.zeros(node.num_children())\n",
    "probs_thresh_accs = np.zeros(node.num_children())\n",
    "\n",
    "\n",
    "for trial in range(node.num_children()):\n",
    "    \n",
    "    print(\"\\ntrial: \", trial)\n",
    "    print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "    \n",
    "    train_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    valid_accs = np.ndarray(shape=(len(gs),len(cs)))\n",
    "    \n",
    "    # get data\n",
    "    test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "    train_valid_idx = np.setdiff1d(np.arange(2500),test_idx)\n",
    "    \n",
    "    full_train_w_X = np.concatenate((train_w_X,train_w_logits[train_valid_idx]))\n",
    "    full_train_w_Y = np.concatenate((np.zeros(1250),np.ones(1250)))\n",
    "\n",
    "    test_idx = np.arange(50*trial,50*(trial+1))\n",
    "    train_valid_idx = np.setdiff1d(np.arange(100),test_idx)\n",
    "\n",
    "    full_valid_w_X = np.concatenate((valid_w_X,valid_w_logits[train_valid_idx]))\n",
    "    full_valid_w_Y = np.concatenate((np.zeros(50),np.ones(50)))    \n",
    "    \n",
    "    full_test_w_X = np.concatenate((test_w_X,test_w_logits[test_idx]))\n",
    "    full_test_w_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "    \n",
    "    # thresholding\n",
    "    logit_thresh, _, logit_thresh_acc = full_lr_line_search(full_train_w_X,full_train_w_Y,\n",
    "                                                      full_test_w_X,full_test_w_Y,probs=False)  \n",
    "    \n",
    "    logit_thresh_accs[trial] = logit_thresh_acc\n",
    "    \n",
    "    probs_thresh, _, probs_thresh_acc = full_lr_line_search(full_train_w_X,full_train_w_Y,\n",
    "                                                      full_test_w_X,full_test_w_Y,probs=True)    \n",
    "    probs_thresh_accs[trial] = probs_thresh_acc\n",
    "    \n",
    "    \n",
    "    # SVM cross validation fitting\n",
    "    for g in range(len(gs)):\n",
    "        for c in range(len(cs)): \n",
    "\n",
    "#             print(\"iter: %d / %d \\t starting g: %.3f and c: %.3f\" % (g*len(cs) + c, len(gs) * len(cs), gs[g],cs[c]))\n",
    "\n",
    "            SVM = SVC(kernel='linear',gamma=gs[g],C=cs[c])\n",
    "            SVM.fit(full_train_w_X,full_train_w_Y)\n",
    "\n",
    "            yhat = SVM.predict(full_train_w_X)\n",
    "            ytrue = full_train_w_Y\n",
    "            conf_mat = confusion_matrix(ytrue,yhat)\n",
    "\n",
    "            train_acc = SVM.score(full_train_w_X,full_train_w_Y)\n",
    "            valid_acc = SVM.score(full_valid_w_X,full_valid_w_Y)\n",
    "\n",
    "            train_accs[g,c] = train_acc\n",
    "            valid_accs[g,c] = valid_acc\n",
    "\n",
    "    row_id = valid_accs.argmax() // len(cs)\n",
    "    col_id = valid_accs.argmax() - (row_id+1)*len(cs)\n",
    "    best_g = gs[row_id]\n",
    "    best_c = cs[col_id]\n",
    "    \n",
    "    SVM = SVC(kernel='linear',gamma=best_g,C=best_c)\n",
    "    SVM.fit(full_train_w_X,full_train_w_Y)\n",
    "\n",
    "    test_acc = SVM.score(full_test_w_X,full_test_w_Y)\n",
    "    \n",
    "    trial_accs[trial] = test_acc\n",
    "    \n",
    "    print(\"\\n valid accs \\n\")\n",
    "    print(valid_accs)\n",
    "    \n",
    "    print(\"best g: \", best_g)\n",
    "    print(\"best c: \", best_c)\n",
    "        \n",
    "    print(\"test acc: \",test_acc)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SVM accs\")\n",
    "print(trial_accs)\n",
    "print(\"mean acc: \",np.mean(trial_accs))\n",
    "w_svm_mean = np.mean(trial_accs)\n",
    "\n",
    "print(\"\\nlogit thresh accs\")\n",
    "print(logit_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(logit_thresh_accs))\n",
    "w_logit_mean = np.mean(logit_thresh_accs)\n",
    "\n",
    "print(\"\\nprobs thresh accs\")\n",
    "print(probs_thresh_accs)\n",
    "print(\"mean acc: \",np.mean(probs_thresh_accs))\n",
    "w_prob_mean = np.mean(probs_thresh_accs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means across class\n",
    "\n",
    "svm_mean = np.mean([np_svm_mean,p_svm_mean,w_svm_mean,e_svm_mean,v_svm_mean])\n",
    "print(\"svm mean: \", svm_mean)\n",
    "logit_mean = np.mean([np_logit_mean,p_logit_mean,w_logit_mean,e_logit_mean,v_logit_mean])\n",
    "print(\"logit mean: \", logit_mean)\n",
    "prob_mean = np.mean([np_prob_mean,p_prob_mean,w_prob_mean,e_prob_mean,v_prob_mean])\n",
    "print(\"prob mean: \", prob_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a specific instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial = 1\n",
    "\n",
    "node = OODroot.get_node(\"weapon\")\n",
    "    \n",
    "print(\"\\ntrial: \", trial)\n",
    "print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "\n",
    "# get data\n",
    "test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "train_valid_idx = np.setdiff1d(np.arange(2500),test_idx)\n",
    "\n",
    "full_train_w_X = np.concatenate((train_w_X,train_w_logits[train_valid_idx]))\n",
    "full_train_w_Y = np.concatenate((np.zeros(1250),np.ones(1250)))\n",
    "\n",
    "test_idx = np.arange(50*trial,50*(trial+1))\n",
    "\n",
    "full_test_w_X = np.concatenate((test_w_X,test_w_logits[test_idx]))\n",
    "full_test_w_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "model = lr_line_search(full_train_w_X,full_train_w_Y)\n",
    "\n",
    "# conf mat\n",
    "yhat = model.predict(full_test_w_X)\n",
    "ytrue = full_test_w_Y\n",
    "conf_mat = confusion_matrix(ytrue,yhat)\n",
    "print(\"\\nconf mat \\n\")\n",
    "print(np.stack([[np.round(x/np.sum(row),2) for x in row] for row in conf_mat]))        \n",
    "\n",
    "\n",
    "yhat = model.predict_proba(test_w_logits[test_idx])[:,1]\n",
    "correct = yhat > 1/2\n",
    "\n",
    "test_acc = np.mean(correct)\n",
    "print(\"test acc: \",test_acc)\n",
    "\n",
    "probs = np.array([np.round(x,2) for x in yhat])\n",
    "\n",
    "print(\"\\nidx of predicted novel:\")\n",
    "yhat_pos = np.where(yhat>1/2)[0]\n",
    "zipped = [x for x in zip(probs[yhat_pos],yhat_pos)]\n",
    "\n",
    "for z in zipped:\n",
    "    print(z)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for trial in [0,1]:#,2,3]:\n",
    "    #trial = 1\n",
    "\n",
    "    node = OODroot.get_node(\"vehicle\")\n",
    "\n",
    "    print(\"\\ntrial: \", trial)\n",
    "    print(\"held out class: %s \\n\" % node.children[trial].name)\n",
    "\n",
    "    # get data\n",
    "    test_idx = np.arange(1250*trial,1250*(trial+1)) # these never actually index anything\n",
    "    train_valid_idx = np.setdiff1d(np.arange(5000),test_idx)\n",
    "\n",
    "    full_train_v_X = np.concatenate((train_v_X,train_v_logits[train_valid_idx]))\n",
    "    full_train_v_Y = np.concatenate((np.zeros(2500),np.ones(3750)))\n",
    "    \n",
    "    test_idx = np.arange(50*trial,50*(trial+1))\n",
    "    \n",
    "    full_test_v_X = np.concatenate((test_v_X,test_v_logits[test_idx]))\n",
    "    full_test_v_Y = np.concatenate((np.zeros(50),np.ones(50)))\n",
    "\n",
    "    model = lr_line_search(full_train_v_X,full_train_v_Y)\n",
    "\n",
    "    # conf mat\n",
    "    yhat = model.predict(full_test_v_X)\n",
    "    ytrue = full_test_v_Y\n",
    "    conf_mat = confusion_matrix(ytrue,yhat)\n",
    "    print(\"\\nconf mat \\n\")\n",
    "    print(np.stack([[np.round(x/np.sum(row),2) for x in row] for row in conf_mat]))        \n",
    "    \n",
    "    \n",
    "    yhat = model.predict_proba(test_v_logits[test_idx])[:,1]\n",
    "    correct = yhat > 1/2\n",
    "\n",
    "    test_acc = np.mean(correct)\n",
    "    print(\"test acc: \",test_acc)\n",
    "\n",
    "    probs = np.array([np.round(x,2) for x in yhat])\n",
    "\n",
    "    print(\"\\nidx of predicted novel:\")\n",
    "    yhat_pos = np.where(yhat>1/2)[0]\n",
    "    zipped = [x for x in zip(probs[yhat_pos],yhat_pos)]\n",
    "\n",
    "    for z in zipped:\n",
    "        print(z)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "\n",
    "print('time: \\t{0:.2f}'.format(end -  start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# means across class\n",
    "\n",
    "svm_mean = np.mean([np_svm_mean,p_svm_mean,w_svm_mean,e_svm_mean,v_svm_mean])\n",
    "print(\"svm mean: \", svm_mean)\n",
    "logit_mean = np.mean([np_logit_mean,p_logit_mean,w_logit_mean,e_logit_mean,v_logit_mean])\n",
    "print(\"logit mean: \", logit_mean)\n",
    "prob_mean = np.mean([np_prob_mean,p_prob_mean,w_prob_mean,e_prob_mean,v_prob_mean])\n",
    "print(\"prob mean: \", prob_mean)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
